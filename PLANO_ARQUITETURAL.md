# üß≠ Plano Arquitetural ‚Äî Books Public API

> Documento do item **5. Plano Arquitetural**: descreve o **pipeline (ingest√£o ‚Üí processamento ‚Üí API ‚Üí consumo)**, a **arquitetura para escalabilidade**, **cen√°rios de uso para cientistas de dados/ML** e o **plano de integra√ß√£o com modelos de ML**.

---

## 1) Vis√£o Geral ‚Äî Pipeline (ingest√£o ‚Üí processamento ‚Üí API ‚Üí consumo)

```mermaid
flowchart LR
    subgraph Ingestao
      A[Scraper - scripts/scrape_books.py] -->|HTTP GET| B[Books to Scrape]
    end

    subgraph Processamento_Armazenamento
      A --> C[CSV bruto - data/books.csv]
      C --> D[ETL leve com pandas - parse e normalizacao]
      D --> E[(Banco de Dados - SQLite ou PostgreSQL)]
      E:::db
    end

    subgraph Exposicao_de_Dados
      F[FastAPI - api/main.py] -->|SQLAlchemy| E
      F --> G[Swagger UI - /docs]
      F --> H[Endpoints REST - /health, /books, /categories, /stats, /top-rated]
    end

    subgraph Consumo
      I[Notebooks - Cientistas de Dados] -->|HTTP| F
      J[Servicos/Apps - Recomendacao ou Backends] -->|HTTP| F
      K[BI/Dashboards] -->|HTTP| F
    end

    subgraph Operacao_Opcional
      L[Scheduler - Render Cron ou Heroku Scheduler] --> A
      M[Admin trigger JWT - /api/v1/scraping/trigger] --> A
      N[Observabilidade - Logs e /metrics]
    end

    classDef db fill:#f7f7f7,stroke:#bbb,stroke-width:1px;
```
**Resumo:** o scraper coleta, o ETL padroniza, o banco persiste, a API exp√µe e clientes consomem via HTTP. Opcionalmente, um scheduler atualiza os dados periodicamente e uma rota protegida pode acionar o scraping manualmente.

---

## 2) Componentes e Responsabilidades

- **Scraper (Ingest√£o)**  
  `scripts/scrape_books.py` coleta t√≠tulo, pre√ßo, rating, disponibilidade, categoria, imagem e detalhes (descri√ß√£o/UPC). Par√¢metro `--limit` pode controlar o volume (para dev).

- **Processamento/Armazenamento**  
  - **CSV** em `data/books.csv` (snapshot compartilh√°vel com o time de dados).  
  - **Banco**: `SQLite` em dev; migr√°vel para **PostgreSQL gerenciado** em produ√ß√£o (melhor concorr√™ncia e durabilidade).  
  - **ORM**: SQLAlchemy define modelos e consultas.

- **API (Exposi√ß√£o)**  
  - **FastAPI** publica endpoints: `/api/v1/health`, `/books`, `/books/{id}`, `/categories`, `/stats/overview`, `/books/top-rated`.  
  - **Swagger UI** em `/docs` para testes e onboarding.  
  - **Valida√ß√µes** com Pydantic / query params paginados e filtros.

- **Consumo**  
  - Cientistas de dados usam `requests`/`pandas.read_json` ou baixam o CSV.  
  - Servi√ßos de recomenda√ß√£o consomem REST (paginado e com filtros) para features on-line.

- **Opera√ß√£o**  
  - Deploy em Render/Heroku/Fly com `uvicorn`.  
  - **Observabilidade**: logs estruturados e `/metrics` (Prometheus) opcional.  
  - **Seguran√ßa**: JWT para rotas de administra√ß√£o (ex.: `/scraping/trigger`).

---

## 3) Arquitetura Pensada para Escalabilidade Futura

**Dados & Banco**  
- Migrar de `SQLite` ‚Üí **PostgreSQL gerenciado** (conex√µes simult√¢neas, backups).  
- √çndices em `title`, `category` e filtros mais usados.  
- Particionamento por categoria/alfab√©tico, se necess√°rio.

**Performance de Leitura**  
- **Cache** (Redis) para respostas quentes: `/stats/overview`, `/books/top-rated` e buscas repetidas.  
- **ETag/Cache-Control** para clientes HTTP.

**Ingest√£o & Processamento**  
- **Workers ass√≠ncronos** (Celery/RQ + Redis) para scraping/ETL mais pesado.  
- **Scheduler** (Render Cron/Heroku Scheduler) para atualiza√ß√µes di√°rias/hor√°rias.  
- **Fila de mensagens** para desacoplar scraping da API.

**Disponibilidade & Resili√™ncia**  
- Inst√¢ncias **stateless** da API (scale-out).  
- Timeouts, retries e circuit breaker para fontes externas.  
- CI/CD com smoke tests e deploy autom√°tico.

**Seguran√ßa**  
- **JWT** para admin; CORS configurado; segredos em vari√°veis de ambiente.  
- Rate limit/API key para consumidores externos, se necess√°rio.  
- TLS/HTTPS no provedor.

---

## 4) Cen√°rios de Uso para Cientistas de Dados / ML

- **Explora√ß√£o**: `/api/v1/books` com filtros (`title`, `category`, `min`, `max`) e pagina√ß√£o para EDA.  
- **Cat√°logo & Segmenta√ß√£o**: `/api/v1/categories` + join local para an√°lises por cluster.  
- **Estat√≠sticas r√°pidas**: `/api/v1/stats/overview` alimenta relat√≥rios e dashboards.  
- **Datasets reproduz√≠veis**: CSV versionado no repo; API como fonte ‚Äúviva‚Äù para atualiza√ß√£o.

Sugest√£o de notebook (pseudo):
```python
import pandas as pd, requests as rq
base = "https://<app>.onrender.com"
df = pd.DataFrame(rq.get(f"{base}/api/v1/books?limit=100").json())
cats = rq.get(f"{base}/api/v1/categories").json()
stats = rq.get(f"{base}/api/v1/stats/overview").json()
```

---

## 5) Plano de Integra√ß√£o com Modelos de ML

**Camada de Features (ML-Ready)**  
- `GET /api/v1/ml/features` ‚Üí features num√©ricas/categ√≥ricas j√° normalizadas/derivadas (ex.: `price_norm`, `rating`, dummies m√≠nimas de categoria).  
- `GET /api/v1/ml/training-data` ‚Üí dataset consolidado para treino (com pagina√ß√£o).

**Servi√ßo de Predi√ß√£o**  
- `POST /api/v1/ml/predictions` ‚Üí recebe payload e retorna predi√ß√£o (MVP: heur√≠stica; produ√ß√£o: modelo `joblib` carregado no startup).  
- **Batch**: job agendado que l√™ do DB, infere e persiste resultados.

**Ciclo de Vida**  
1. **MVP**: heur√≠stica ‚Üí cumpre o b√¥nus rapidamente.  
2. **Treino offline** (notebooks/pipelines) ‚Üí artefato versionado.  
3. **Registry leve** (S3/GCS/Git).  
4. **Deploy**: API carrega o artefato (hash/vers√£o).  
5. **Observabilidade de modelo**: lat√™ncia, taxa de erro, drift de entradas.  
6. **Retreino**: agendado (cron) ou por gatilho de drift.

---

## 6) Opera√ß√£o em Produ√ß√£o

- **Web Service**: `uvicorn api.main:app --host 0.0.0.0 --port ${PORT}`.  
- **Dados**: pr√©-carregados (CSV/DB) no repo ou atualizados via scheduler.  
- **Health**: `/api/v1/health` deve retornar `status=ok` e `books>0`.  
- **Monitoramento**: logs no provedor e `/metrics` (se habilitado).  
- **Seguran√ßa**: vari√°veis de ambiente (`DATABASE_URL`, `SECRET_KEY`, etc.).

---

## 7) Roadmap de Evolu√ß√£o

1. Migrar DB para **PostgreSQL** e adicionar **migrations**.  
2. **Cache Redis** para endpoints de insights.  
3. **Workers** para scraping/ETL ass√≠ncronos.  
4. **Endpoints ML-Ready** completos e primeiro modelo real (recomenda√ß√£o/similaridade).  
5. Observabilidade (Prometheus/Grafana) e dashboards.  
6. JWT + RBAC b√°sico para opera√ß√µes administrativas.

---

> Este plano √© auto-contido e pode ser inclu√≠do no README ou entregue como documento separado.
